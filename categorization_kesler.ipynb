{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5893604b-9ebb-4a07-ba2d-30c5b49abb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6fc0332926455694db0589decea5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64c20afae33413b8f19deb01657526b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery df_test\n",
    "\n",
    "SELECT *, CONCAT('Name: \\n ', name, ' \\n ', \n",
    "                 \"Description: \\n \", description, ' \\n ',\n",
    "                 \"Labels: \\n \", TO_JSON_STRING(vision_api_labels)\n",
    "                ) as attr FROM solutions-2023-mar-107.mercari.13K_synthetic_attributes_embeddings TABLESAMPLE SYSTEM (1 PERCENT)\n",
    "WHERE rand() < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4cdcee5d-b7b8-40c4-a348-6081e81d5651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009e6bb00a534471968de7e188d3a03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd447587a8d4c0bb6d8b868e904e909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery df_train\n",
    "\n",
    "SELECT *, CONCAT('Name: \\n ', name, ' \\n ', \n",
    "                 \"Description: \\n \", description, ' \\n ',\n",
    "                 \"Labels: \\n \", TO_JSON_STRING(vision_api_labels)\n",
    "                ) as attr FROM solutions-2023-mar-107.mercari.13K_synthetic_attributes_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c58788c1-b357-4799-b2c4-939405078248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13450"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c448db97-b78c-446b-94b7-61b49e4fffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13450\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4f0e2b7f-fb8d-485e-9e66-d99cced5ccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55b5a84cf1646fa92e0ee6ca885c36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e0c0443bb643e584293532bcbbc97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery category_df\n",
    "select distinct(c0_name) from `solutions-2023-mar-107.mercari.13K_synthetic_attributes_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7dac72b5-c3b6-4935-9f92-756024271e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6c4d2b87-d6d6-44aa-8341-6cae9b852f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Home': 0, 'Books': 1, 'Toys & Collectibles': 2, 'Men': 3, 'Women': 4, 'Beauty': 5, 'Vintage & collectibles': 6, 'Kids': 7, 'Electronics': 8, 'Sports & outdoors': 9, 'Handmade': 10, 'Arts & Crafts': 11, 'Other': 12, 'Pet Supplies': 13, 'Garden & Outdoor': 14, 'Office': 15, 'Tools': 16}\n"
     ]
    }
   ],
   "source": [
    "# get list of unique categories\n",
    "\n",
    "categories = {}\n",
    "category_id = 0\n",
    "for index, row in category_df.iterrows():\n",
    "    categories[row['c0_name']] = index\n",
    "        \n",
    "ncategories = len(categories)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4be5bab7-d288-4928-8753-df5dfac09df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\nimport random\\n\\nnfeatures=1000\\nvectorizer = HashingVectorizer(n_features=nfeatures, stop_words=\\'english\\', norm = None)\\n\\n# Convert data to Kesler\\'s Construction\\ndef convert_to_kesler(description, category_i):\\n    x_vector = vectorizer.transform([description]).toarray()[0].tolist()\\n    y_position = category_i\\n    xy_vector = [0]*(y_position)*nfeatures + x_vector + [0]*(ncategories-(y_position+1))*nfeatures\\n    return (xy_vector)\\n\\nX_train = []\\ny_train = []\\nfor index, row in df_train.iterrows():\\n    \\n    # add positive example\\n    category=categories[row[\"c0_name\"]]\\n    new_row = convert_to_kesler(row[\"attr\"],category)\\n    X_train.append(new_row)\\n    y_train.append(1)\\n                        \\n    # add negative example\\n    random_number=category                                                    \\n    while random_number == category:\\n        random_number = random.randint(0, ncategories-1)                                              \\n    new_row = convert_to_kesler(row[\"attr\"],random_number)\\n    X_train.append(new_row)\\n    y_train.append(0)\\n    \\n    # add another negative example\\n    random_number=category                                                    \\n    while random_number == category:\\n        random_number = random.randint(0, ncategories-1)                                              \\n    new_row = convert_to_kesler(row[\"attr\"],random_number)\\n    X_train.append(new_row)\\n    y_train.append(0)\\n'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import random\n",
    "\n",
    "nfeatures=1000\n",
    "vectorizer = HashingVectorizer(n_features=nfeatures, stop_words='english', norm = None)\n",
    "\n",
    "# Convert data to Kesler's Construction\n",
    "def convert_to_kesler(description, category_i):\n",
    "    x_vector = vectorizer.transform([description]).toarray()[0].tolist()\n",
    "    y_position = category_i\n",
    "    xy_vector = [0]*(y_position)*nfeatures + x_vector + [0]*(ncategories-(y_position+1))*nfeatures\n",
    "    return (xy_vector)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    # add positive example\n",
    "    category=categories[row[\"c0_name\"]]\n",
    "    new_row = convert_to_kesler(row[\"attr\"],category)\n",
    "    X_train.append(new_row)\n",
    "    y_train.append(1)\n",
    "                        \n",
    "    # add negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_kesler(row[\"attr\"],random_number)\n",
    "    X_train.append(new_row)\n",
    "    y_train.append(0)\n",
    "    \n",
    "    # add another negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_kesler(row[\"attr\"],random_number)\n",
    "    X_train.append(new_row)\n",
    "    y_train.append(0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2e46cb40-ac8d-42f2-a0bd-c6964a46053a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100\n",
      "iter 200\n",
      "iter 300\n",
      "iter 400\n",
      "iter 500\n",
      "iter 600\n",
      "iter 700\n",
      "iter 800\n",
      "iter 900\n",
      "iter 1000\n",
      "iter 1100\n",
      "iter 1200\n",
      "iter 1300\n",
      "iter 1400\n",
      "iter 1500\n",
      "iter 1600\n",
      "iter 1700\n",
      "iter 1800\n",
      "iter 1900\n",
      "iter 2000\n",
      "iter 2100\n",
      "iter 2200\n",
      "iter 2300\n",
      "iter 2400\n",
      "iter 2500\n",
      "iter 2600\n",
      "iter 2700\n",
      "iter 2800\n",
      "iter 2900\n",
      "iter 3000\n",
      "iter 3100\n",
      "iter 3200\n",
      "iter 3300\n",
      "iter 3400\n",
      "iter 3500\n",
      "iter 3600\n",
      "iter 3700\n",
      "iter 3800\n",
      "iter 3900\n",
      "iter 4000\n",
      "iter 4100\n",
      "iter 4200\n",
      "iter 4300\n",
      "iter 4400\n",
      "iter 4500\n",
      "iter 4600\n",
      "iter 4700\n",
      "iter 4800\n",
      "iter 4900\n",
      "iter 5000\n",
      "iter 5100\n",
      "iter 5200\n",
      "iter 5300\n",
      "iter 5400\n",
      "iter 5500\n",
      "iter 5600\n",
      "iter 5700\n",
      "iter 5800\n",
      "iter 5900\n",
      "iter 6000\n",
      "iter 6100\n",
      "iter 6200\n",
      "iter 6300\n",
      "iter 6400\n",
      "iter 6500\n",
      "iter 6600\n",
      "iter 6700\n",
      "iter 6800\n",
      "iter 6900\n",
      "iter 7000\n",
      "iter 7100\n",
      "iter 7200\n",
      "iter 7300\n",
      "iter 7400\n",
      "iter 7500\n",
      "iter 7600\n",
      "iter 7700\n",
      "iter 7800\n",
      "iter 7900\n",
      "iter 8000\n",
      "iter 8100\n",
      "iter 8200\n",
      "iter 8300\n",
      "iter 8400\n",
      "iter 8500\n",
      "iter 8600\n",
      "iter 8700\n",
      "iter 8800\n",
      "iter 8900\n",
      "iter 9000\n",
      "iter 9100\n",
      "iter 9200\n",
      "iter 9300\n",
      "iter 9400\n",
      "iter 9500\n",
      "iter 9600\n",
      "iter 9700\n",
      "iter 9800\n",
      "iter 9900\n",
      "iter 10000\n",
      "iter 10100\n",
      "iter 10200\n",
      "iter 10300\n",
      "iter 10400\n",
      "iter 10500\n",
      "iter 10600\n",
      "iter 10700\n",
      "iter 10800\n",
      "iter 10900\n",
      "iter 11000\n",
      "iter 11100\n",
      "iter 11200\n",
      "iter 11300\n",
      "iter 11400\n",
      "iter 11500\n",
      "iter 11600\n",
      "iter 11700\n",
      "iter 11800\n",
      "iter 11900\n",
      "iter 12000\n",
      "iter 12100\n",
      "iter 12200\n",
      "iter 12300\n",
      "iter 12400\n",
      "iter 12500\n",
      "iter 12600\n",
      "iter 12700\n",
      "iter 12800\n",
      "iter 12900\n",
      "iter 13000\n",
      "iter 13100\n",
      "iter 13200\n",
      "iter 13300\n",
      "iter 13400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "\n",
    "nfeatures=1408*2\n",
    "\n",
    "# Convert data to Kesler's Construction\n",
    "def convert_to_vector(embedding, category_i):\n",
    "    x_vector = embedding#.tolist()\n",
    "    y_position = category_i\n",
    "    xy_vector = [0]*(y_position)*nfeatures + x_vector + [0]*(ncategories-(y_position+1))*nfeatures\n",
    "    if len(xy_vector) != 23936*2:\n",
    "        print(len(xy_vector))\n",
    "        return None\n",
    "    xy_vector = [x+1 for x in xy_vector]\n",
    "    return (xy_vector)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "count=0\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    count+=1\n",
    "    if count%500==0:\n",
    "        print (\"iter \" + str(count))\n",
    "        \n",
    "    # add positive example\n",
    "    category=categories[row[\"c0_name\"]]\n",
    "    new_row = convert_to_vector(row[\"text_embedding\"].tolist()+row[\"image_embedding\"].tolist(),category)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(1)\n",
    "                        \n",
    "    # add negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_vector(row[\"text_embedding\"].tolist()+row[\"image_embedding\"].tolist(),random_number)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(0)\n",
    "    \n",
    "    # add another negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_vector(row[\"text_embedding\"].tolist()+row[\"image_embedding\"].tolist(),random_number)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(0)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "80975cc7-f5c3-4b45-a497-bcc96a9f3e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 500\n",
      "iter 1000\n",
      "iter 1500\n",
      "iter 2000\n",
      "iter 2500\n",
      "iter 3000\n",
      "iter 3500\n",
      "iter 4000\n",
      "iter 4500\n",
      "iter 5000\n",
      "iter 5500\n",
      "iter 6000\n",
      "iter 6500\n",
      "iter 7000\n",
      "iter 7500\n",
      "iter 8000\n",
      "iter 8500\n",
      "iter 9000\n",
      "iter 9500\n",
      "iter 10000\n",
      "iter 10500\n",
      "iter 11000\n",
      "iter 11500\n",
      "iter 12000\n",
      "iter 12500\n",
      "iter 13000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "\n",
    "import random\n",
    "\n",
    "nfeatures=768\n",
    "\n",
    "# Convert data to Kesler's Construction\n",
    "def convert_to_vector(description, category_i):\n",
    "    x_vector = model.get_embeddings([description])[0].values\n",
    "    y_position = category_i\n",
    "    xy_vector = [0]*(y_position)*nfeatures + x_vector + [0]*(ncategories-(y_position+1))*nfeatures\n",
    "    if len(xy_vector) != nfeatures*ncategories:\n",
    "        print(len(xy_vector))\n",
    "        print(len(x_vector))\n",
    "        return None\n",
    "    return (xy_vector)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    count+=1\n",
    "    if count%500==0:\n",
    "        print (\"iter \" + str(count))\n",
    "    \n",
    "    # add positive example\n",
    "    category=categories[row[\"c0_name\"]]\n",
    "    new_row = convert_to_vector(row[\"attr\"],category)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(1)\n",
    "                        \n",
    "    # add negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_vector(row[\"attr\"],random_number)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(0)\n",
    "    \n",
    "    # add another negative example\n",
    "    random_number=category                                                    \n",
    "    while random_number == category:\n",
    "        random_number = random.randint(0, ncategories-1)                                              \n",
    "    new_row = convert_to_vector(row[\"attr\"],random_number)\n",
    "    if new_row is not None:\n",
    "        X_train.append(new_row)\n",
    "        y_train.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "729344b0-714b-4f71-b078-12fcbc46073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "#clf = MultinomialNB()\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "#clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=0.0001, max_iter=3000, tol=None, shuffle=True, verbose=0, learning_rate='adaptive', eta0=0.01, early_stopping=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "05a25443-f491-4949-a064-070a99de1a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf.partial_fit(X_train, y_train, classes=[0,1])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f5980318-4503-4046-bf2a-4e3781e8f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11724137931034483\n",
      "f1: 0.045700350817147246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def predict(line):\n",
    "    picked_cat = -1\n",
    "    max_prob = 0\n",
    "    for key in categories:\n",
    "        pred = clf.predict_proba([convert_to_vector(line[\"text_embedding\"].tolist(), categories[key])])[0][1]\n",
    "        if pred > max_prob:\n",
    "            max_prob = pred\n",
    "            picked_cat = key\n",
    "    return picked_cat\n",
    "\n",
    "#model accuracy\n",
    "df_test[\"predicted_category\"] = df_test.apply(predict, axis=1)\n",
    "\n",
    "\n",
    "y_true = df_test[\"c0_name\"].tolist()\n",
    "y_pred = df_test[\"predicted_category\"].tolist()\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"f1: {f1_score(y_true, y_pred,average='weighted')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1c7d62e2-f2a9-4900-b3bf-ba021dab70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_category      Men  Women\n",
      "c0_name                           \n",
      "Beauty                    4      1\n",
      "Books                     1      0\n",
      "Electronics               5      0\n",
      "Handmade                  1      0\n",
      "Home                      8      0\n",
      "Kids                      8      1\n",
      "Men                      15      0\n",
      "Office                    1      0\n",
      "Other                     4      0\n",
      "Sports & outdoors         2      0\n",
      "Toys & Collectibles      31      0\n",
      "Vintage & collectibles    5      0\n",
      "Women                    56      2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.crosstab(df_test['c0_name'], df_test['predicted_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "379c141e-183d-4b9b-b639-c23264c9034c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Men', 'Women'], dtype=object)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"predicted_category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6b03b6bb-13e3-485d-86c4-0a4863e48b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c0_name\n",
       "Women                     5693\n",
       "Toys & Collectibles       2010\n",
       "Men                       1420\n",
       "Home                       950\n",
       "Kids                       854\n",
       "Electronics                656\n",
       "Vintage & collectibles     520\n",
       "Beauty                     507\n",
       "Books                      298\n",
       "Other                      146\n",
       "Sports & outdoors          121\n",
       "Handmade                   120\n",
       "Office                      47\n",
       "Arts & Crafts               42\n",
       "Pet Supplies                31\n",
       "Garden & Outdoor            18\n",
       "Tools                       17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"c0_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1f29e914-a32e-4205-8f41-d06e032dbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test[\"c0_name\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "858485e6-2b11-4e24-b9c3-070c2740ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[\"c0_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b33bd56e-7673-42d3-9210-1690ec5fa481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import random\n",
    "\n",
    "nfeatures=3000\n",
    "documents = df_train[\"attr\"].tolist()\n",
    "vectorizer = HashingVectorizer(n_features=nfeatures, stop_words='english', binary=True)\n",
    "\n",
    "# Convert data to Kesler's Construction\n",
    "def convert_to_kesler(description, category_i):\n",
    "    x_vector = vectorizer.transform([description]).toarray()[0].tolist()\n",
    "    y_position = category_i\n",
    "    xy_vector = x_vector\n",
    "    return (xy_vector)\n",
    "\"\"\"\n",
    "\n",
    "X_train = []\n",
    "y_train_ = []\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    # add positive example\n",
    "    #category=categories[row[\"c0_name\"]]\n",
    "    #new_row = convert_to_kesler(row[\"attr\"],category)\n",
    "    #X_train.append(new_row)\n",
    "    y_train_.append(row[\"c0_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e12447a2-4be2-404a-8434-95dcf24b5330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "23a24d69-36ec-4bfe-9873-5495fd23a38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m base_lr \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m      4\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m OneVsRestClassifier(base_lr)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mclf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/multiclass.py:339\u001b[0m, in \u001b[0;36mOneVsRestClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    335\u001b[0m columns \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# In cases where individual estimators are very fast to train setting\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of spawning threads.  See joblib issue #112.\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_binary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_binarizer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_binarizer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/multiclass.py:90\u001b[0m, in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m clone(estimator)\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "base_lr = LogisticRegression()\n",
    "clf2 = OneVsRestClassifier(base_lr)\n",
    "\n",
    "clf2.fit(X_train, y_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7694fcd7-283f-4cbb-b27b-3e3058f0122c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import f1_score\\n\\ndef predict(line):\\n    pred = clf2.predict([convert_to_kesler(line[\"attr\"],0)])\\n    return pred\\n\\n#model accuracy\\ndf_test[\"predicted_category\"] = df_test.apply(predict, axis=1)\\n\\ny_true = df_test[\"c0_name\"].tolist()\\ny_pred = df_test[\"predicted_category\"].tolist()\\n\\nprint(type(y_true))\\n\\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\\nprint(f\"f1: {f1_score(y_true, y_pred,average=\\'weighted\\')}\")\\n'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict(line):\n",
    "    pred = clf2.predict([convert_to_vector(line[\"attr\"],0)])\n",
    "    return pred\n",
    "\n",
    "#model accuracy\n",
    "df_test[\"predicted_category\"] = df_test.apply(predict, axis=1)\n",
    "\n",
    "y_true = df_test[\"c0_name\"].tolist()\n",
    "y_pred = df_test[\"predicted_category\"].tolist()\n",
    "\n",
    "print(type(y_true))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"f1: {f1_score(y_true, y_pred,average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8139cb-3f04-4db7-82ee-8a126d3a18c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8425371-aacc-48fb-8651-0d78b4d3edfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
